{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('bot_tmp.log', encoding='utf-8') as f:\n",
    "#     lines = [line.split(\" --- \") for line in f.readlines()]\n",
    "\n",
    "#     res = {'files': []}\n",
    "\n",
    "#     for idx, line in enumerate(lines):\n",
    "#         entry = {\n",
    "#             'id': idx + 1,\n",
    "#             'type': line[1].strip()[1:-1],\n",
    "#             'name': line[0].strip()[1:-1],\n",
    "#             'uploaded_date': line[2].strip()[1:-1]\n",
    "#         }\n",
    "\n",
    "#         res['files'].append(entry)\n",
    "\n",
    "#     with open('files_info.json', 'w', encoding='utf-8') as f:\n",
    "#         json.dump(res, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING -> This doesn't work reliable.... It deletes data that are matched!!!!\n",
    "# import json\n",
    "\n",
    "# auxiliaryList = []\n",
    "\n",
    "# with open('../files_info.json', 'r+', encoding='utf-8') as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "#     for info in data[\"files\"]:\n",
    "#         info[\"id\"] = 1\n",
    "\n",
    "#     for idx, info in enumerate(data[\"files\"]):\n",
    "#         if info not in auxiliaryList:\n",
    "#             auxiliaryList.append(info)\n",
    "\n",
    "#     for idx, info in enumerate(auxiliaryList):\n",
    "#         info[\"id\"] = idx + 1\n",
    "\n",
    "#     print(len(data['files']))\n",
    "    \n",
    "# with open('../files_info.json', 'w', encoding='utf-8') as f:\n",
    "#     data[\"files\"] = auxiliaryList\n",
    "#     json.dump(data, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "import string\n",
    "import platform\n",
    "import unicodedata\n",
    "from selenium import webdriver\n",
    "from json.decoder import JSONDecodeError\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "# Packages & Libraries\n",
    "import json\n",
    "from json.decoder import JSONDecodeError\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from constants import (\n",
    "    secrets,\n",
    "    encoding,\n",
    "    target_file_type,\n",
    "    log_file_location,\n",
    "    normalization_form,\n",
    "    explicit_wait_time,\n",
    "    tracker_file_location,\n",
    "    network_failure_timeout,\n",
    ")\n",
    "\n",
    "from helperFunctions import (\n",
    "    login,\n",
    "    updateLog,\n",
    "    waitNSeconds,\n",
    "    downloadFile,\n",
    "    loadMoreFiles,\n",
    "    appendFilesInfo,\n",
    "    initializeWebpage,\n",
    "    checkDownloadStatus,\n",
    "    initializeWebDriver,\n",
    "    waitToFinishDownload,\n",
    "    getExistingFilesInfo,\n",
    ")\n",
    "\n",
    "driver, wait = initializeWebDriver()\n",
    "login(driver, wait)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializeWebpage(driver, \"https://www.facebook.com/groups/201623576939858/files/\")\n",
    "\"\"\"\n",
    " targets identifiers -> this identifiers will change continuously so update it according to your needs \n",
    "\n",
    "\"\"\"\n",
    "download_button_cssSelector = \"a[href*='https://www.facebook.com/download/']\"\n",
    "permalink_xpath = \"//a[contains(@href, 'https://www.facebook.com/groups/201623576939858/permalink/')]\"  # by default finds 15 per-scroll\n",
    "fileOption_xpath = \"//div[@aria-label='File options']\"  # by default finds 15 per-scroll\n",
    "\n",
    "# Target identifiers\n",
    "# They change too often so they have to be present here\n",
    "fileName_xpath = \"//span[@class='d2edcug0 hpfvmrgz qv66sw1b c1et5uql lr9zc1uh a8c37x1j fe6kdd0r mau55g9w c8b282yb keod5gw0 nxhoafnm aigsh9s9 d3f4x2em iv3no6db jq4qci2q a3bd9o3v lrazzd5p oo9gr5id hzawbc8m']\"  # find 15 per-scroll\n",
    "fileTypeDate_xpath = \"//span[@class='d2edcug0 hpfvmrgz qv66sw1b c1et5uql lr9zc1uh a8c37x1j fe6kdd0r mau55g9w c8b282yb keod5gw0 nxhoafnm aigsh9s9 d9wwppkn iv3no6db e9vueds3 j5wam9gi b1v8xokw oo9gr5id hzawbc8m']\"  # find 30 per-scroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compareString(s1, s2):\n",
    "#     # string will be normalized before coming here\n",
    "#     # s1 = unicodedata.normalize(normalization_form, s1)\n",
    "#     # s2 = unicodedata.normalize(normalization_form, s2)\n",
    "\n",
    "#     # Removing whitespace in the string before comparing\n",
    "#     # because when file saves in the machine it seems to add whitepsaces after '-'\n",
    "#     # remove = string.punctuation + string.whitespace\n",
    "#     remove = string.whitespace\n",
    "#     mapping = {ord(c): None for c in remove}\n",
    "\n",
    "#     return s1.translate(mapping) == s2.translate(mapping)\n",
    "\n",
    "\n",
    "# def getExistingFilesInfo():\n",
    "#     with open(tracker_file_location, \"r\", encoding=encoding) as f:\n",
    "#         try:\n",
    "#             files_info = json.load(f)[\"files\"]\n",
    "#         except JSONDecodeError:\n",
    "#             files_info = []\n",
    "\n",
    "#     tracked_files = sorted(\n",
    "#         [\n",
    "#             [\n",
    "#                 unicodedata.normalize(normalization_form, info[\"uploaded_date\"]),\n",
    "#                 unicodedata.normalize(normalization_form, info[\"name\"]),\n",
    "#             ]\n",
    "#             for info in files_info\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     return [], tracked_files\n",
    "\n",
    "\n",
    "# def binarySearch(item, itemList, multipleCheck=False):\n",
    "#     left = 0\n",
    "#     right = len(itemList) - 1\n",
    "\n",
    "#     while left <= right:\n",
    "#         mid = left + (right - left) // 2\n",
    "\n",
    "#         if multipleCheck:\n",
    "#             if compareString(itemList[mid][0], item[0]) and compareString(\n",
    "#                 itemList[mid][1], item[1]\n",
    "#             ):\n",
    "#                 return mid\n",
    "#             elif itemList[mid][0] > item[0]:\n",
    "#                 right = mid - 1\n",
    "#             else:\n",
    "#                 left = mid + 1\n",
    "#         else:\n",
    "#             if compareString(itemList[mid], item):\n",
    "#                 return mid\n",
    "#             elif itemList[mid] > item:\n",
    "#                 right = mid - 1\n",
    "#             else:\n",
    "#                 left = mid + 1\n",
    "\n",
    "#     return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tracker_file_location, \"r\", encoding=encoding) as f:\n",
    "    try:\n",
    "        registered_files = json.load(f)[\"files\"]\n",
    "    except JSONDecodeError:\n",
    "        registered_files = []\n",
    "\n",
    "def compareString(s1, s2):\n",
    "    s1 = unicodedata.normalize(normalization_form, s1)\n",
    "    s2 = unicodedata.normalize(normalization_form, s2)\n",
    "\n",
    "    remove = string.whitespace\n",
    "    mapping = {ord(c): None for c in remove}\n",
    "\n",
    "    return s1.translate(mapping) == s2.translate(mapping)\n",
    "\n",
    "\n",
    "def searchFile(match_item, itemList, multipleCheck=False):\n",
    "    if multipleCheck:\n",
    "        for idx, item in enumerate(itemList):\n",
    "            if compareString(match_item[\"name\"], item[\"name\"]) and compareString(\n",
    "                match_item[\"uploaded_date\"], item[\"uploaded_date\"]\n",
    "            ):\n",
    "                return idx\n",
    "\n",
    "    else:\n",
    "        for idx, item in enumerate(itemList):\n",
    "            if compareString(item, match_item):\n",
    "                return idx\n",
    "\n",
    "    return -1\n",
    "\n",
    "\n",
    "def loadMoreFiles(driver, files_to_load, identifier, timeout=600, n_scroll=1):\n",
    "    for _ in range(n_scroll):  # do this operation(scroll to load) for n times\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        time_elapsed = 0\n",
    "        current_len = len(files_to_load[0])\n",
    "\n",
    "        ########################################################################\n",
    "        # If the first one is loaded then all others will surely get loaded    #\n",
    "        # so you don't have to check the whole length of 'files_to_load' array #\n",
    "        # but if you want you can do something like this ->                    #\n",
    "        # current_len = sum(len(i) for i in files_to_load)                     #\n",
    "        ########################################################################\n",
    "        while (current_len >= len(files_to_load[0])) and time_elapsed < timeout:\n",
    "            waitNSeconds(1)\n",
    "\n",
    "            # iterate through all the files that are needed to be loaded\n",
    "            for idx, _ in enumerate(files_to_load):\n",
    "                files_to_load[idx].extend(\n",
    "                    [\n",
    "                        element\n",
    "                        for element in driver.find_elements(By.XPATH, identifier[idx])\n",
    "                        if element not in files_to_load[idx]\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "            time_elapsed += 1\n",
    "\n",
    "    return files_to_load\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j, k = 0, 1\n",
    "\n",
    "# Refresh existing files info every time this cell loads\n",
    "# downloaded_files, tracked_files = getExistingFilesInfo()\n",
    "\n",
    "# Grabbing initial loadeded target\n",
    "try:\n",
    "    file_option_buttons = wait.until(\n",
    "        EC.presence_of_all_elements_located((By.XPATH, fileOption_xpath))\n",
    "    )\n",
    "    file_names = wait.until(\n",
    "        EC.presence_of_all_elements_located((By.XPATH, fileName_xpath))\n",
    "    )\n",
    "    file_types_and_dates = wait.until(\n",
    "        EC.presence_of_all_elements_located((By.XPATH, fileTypeDate_xpath))\n",
    "    )\n",
    "\n",
    "    # This is the unique identifier of a post/file\n",
    "    post_permalink = wait.until(\n",
    "        EC.presence_of_all_elements_located((By.XPATH, permalink_xpath))\n",
    "    )\n",
    "except TimeoutException:\n",
    "    print(\"XPATH's identifier (class names) has changed again! ðŸ¤¯\")\n",
    "\n",
    "\n",
    "\"\"\" MAIN LOOP \"\"\"\n",
    "for idx, button in enumerate(file_option_buttons):\n",
    "    try:\n",
    "        _name = file_names[idx].text\n",
    "        _type = file_types_and_dates[j].text\n",
    "        _date = file_types_and_dates[k].text\n",
    "        _permalink = post_permalink[idx].get_attribute(\"href\")\n",
    "\n",
    "        # as 'file_types_and_dates' contains both file-type and date\n",
    "        j += 2\n",
    "        k += 2\n",
    "\n",
    "        # Scrolling after it reaches at the end of the list to load more files\n",
    "        if button is file_option_buttons[-1]:\n",
    "            [\n",
    "                file_option_buttons,\n",
    "                file_names,\n",
    "                file_types_and_dates,\n",
    "                post_permalink,\n",
    "            ] = loadMoreFiles(\n",
    "                driver,\n",
    "                [file_option_buttons, file_names, file_types_and_dates, post_permalink],\n",
    "                [fileOption_xpath, fileName_xpath, fileTypeDate_xpath, permalink_xpath],\n",
    "            )\n",
    "\n",
    "            updateLog(\"\\nTotal Loaded Files: {}\\n\".format(len(file_option_buttons)))\n",
    "\n",
    "        \"\"\"\n",
    "        Download pdf files only\n",
    "        Cannot check this before the scroll because\n",
    "        here is a possibility to have more than one page worth of non-pdf files\n",
    "        \n",
    "        \"\"\"\n",
    "        if not _type == target_file_type.upper():\n",
    "            updateLog(\n",
    "                '\\nðŸ˜ª Skipping ({}): \"{} --- {}\", ðŸ¤” Reason: FILE_TYPE: \"{}\"'.format(\n",
    "                    idx + 1, _name, _date, _type\n",
    "                )\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        isExists = searchFile(\n",
    "            {\"name\": _name, \"uploaded_date\": _date}, registered_files, True\n",
    "        )\n",
    "\n",
    "        if isExists != -1:\n",
    "            # waitNSeconds(0.5)\n",
    "\n",
    "            with open(tracker_file_location, \"r+\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "                for tidx, info in enumerate(data[\"files\"]):\n",
    "                    if compareString(info[\"name\"], _name) and compareString(\n",
    "                        info[\"uploaded_date\"], _date\n",
    "                    ):\n",
    "                        if \"post_permalink\" in info:\n",
    "                            updateLog(\n",
    "                                \"\\nSkipped -> post_permalink already exists: {} -- {}\".format(\n",
    "                                    _name, _date\n",
    "                                )\n",
    "                            )\n",
    "                            break\n",
    "\n",
    "                        data[\"files\"][tidx] = {\n",
    "                            \"id\": info[\"id\"],\n",
    "                            \"type\": info[\"type\"],\n",
    "                            \"name\": info[\"name\"],\n",
    "                            \"uploaded_date\": info[\"uploaded_date\"],\n",
    "                            \"post_permalink\": _permalink,\n",
    "                        }\n",
    "\n",
    "                        updateLog(\n",
    "                            \"\\n*** Info updated: {} -- {} -- {}\".format(\n",
    "                                info[\"id\"], _name, _date\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                        f.seek(0)\n",
    "                        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "                        data.clear()\n",
    "\n",
    "                        # isExists = -2\n",
    "                        break\n",
    "\n",
    "            registered_files.pop(isExists)\n",
    "\n",
    "            # if isExists == -2:\n",
    "            #     break\n",
    "\n",
    "            if not registered_files:\n",
    "                updateLog(\"\\n\\n\\n\\nALLLLLLLLLL DONE\\n\\n\\n\")\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            updateLog(\"\\nSkipped: {} -- {}\".format(_name, _date))\n",
    "        print(\"Progress:\", idx + 1)\n",
    "\n",
    "    except Exception as e:\n",
    "        updateLog(\"\\n*** ERROR at {}, date: {} ***\\n--> {}\\n\".format(_name, _date, e))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tracker_file_location, \"r+\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    chk_len = len(data[\"files\"][1])\n",
    "\n",
    "    for idx, info in enumerate(data[\"files\"]):\n",
    "        if len(info) != chk_len:\n",
    "            print(info[\"id\"])\n",
    "            # count += 1\n",
    "    \n",
    "    # print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5733 SED\n"
     ]
    }
   ],
   "source": [
    "with open(tracker_file_location, \"r+\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)[\"files\"]\n",
    "    # size = len(data) - 1\n",
    "\n",
    "    name = []\n",
    "    nameSet = set()\n",
    "\n",
    "    for i in data:\n",
    "        name.append(i[\"name\"] + i[\"uploaded_date\"])\n",
    "        nameSet.add(i[\"name\"] + i[\"uploaded_date\"])\n",
    "    \n",
    "    name = sorted(name)\n",
    "\n",
    "    size1 = len(name)\n",
    "    size2 = len(nameSet)\n",
    "\n",
    "    print(size1, \"SED\")\n",
    "\n",
    "    # array = []\n",
    "\n",
    "    # for i in range(0, size):\n",
    "    #     tmp = set()\n",
    "\n",
    "    #     for j in range(1, size):\n",
    "    #         if compareString(data[i][\"name\"], data[j][\"name\"]):\n",
    "    #             tmp.add(data[i][\"id\"])\n",
    "    #             tmp.add(data[j][\"id\"])\n",
    "\n",
    "    #     array.append(tmp)\n",
    "\n",
    "    # print(\"**\", len(array), \"**\")\n",
    "\n",
    "    # for a in array:\n",
    "    #     print(a)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "88901c317dc70daf78dfd7a773a6525c4dfeb1fa8e4740ded1ab17187716d835"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
