{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "############### Acknowledgement ###############\n",
    "1. set your browser download location to your desire location because all files will be downloaded to the download location setted in your browser.\n",
    "\n",
    "# Thanks -> pythonjar, MariyaSha and some other stackoverflow members...\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable all alerts from facebook while scrapping\n",
    "options = webdriver.ChromeOptions()\n",
    "prefs = {\"profile.default_content_setting_values.notifications\" : 2}\n",
    "options.add_experimental_option(\"prefs\",prefs)\n",
    "\n",
    "\"\"\"\n",
    "#############################################################################\n",
    "# specify the path to chromedriver.exe (download and save on your computer) #\n",
    "#############################################################################\n",
    "\n",
    "############################## WINDOWS ######################################\n",
    "# service = Service('C:/Users/shahr/chromedriver.exe')                      #\n",
    "# driver = webdriver.Chrome(service=service, options=options)               #\n",
    "#############################################################################\n",
    "\n",
    "############################## LINUX ########################################\n",
    "# driver = webdriver.Chrome('chromedriver', options=options) # linux        #\n",
    "#############################################################################\n",
    "\n",
    "\"\"\"\n",
    "service = Service('C:/Users/shahr/chromedriver.exe')\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# for obvious reason ðŸ˜\n",
    "with open('.secret', encoding='utf-8') as f:\n",
    "    secrets = [line.strip() for line in f.readlines()] # 0 -> username, 1 -> password, 2 -> download directory (for checking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the webpage\n",
    "driver.get(\"http://www.facebook.com\")\n",
    "\n",
    "# target credentials\n",
    "username = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name='email']\")))\n",
    "password = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name='pass']\")))\n",
    "\n",
    "# enter username and password\n",
    "username.clear()\n",
    "username.send_keys(secrets[0])\n",
    "password.clear()\n",
    "password.send_keys(secrets[1])\n",
    "\n",
    "# target thesrc login button and click it\n",
    "button = WebDriverWait(driver, 2).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[type='submit']\"))).click()\n",
    "\n",
    "\"\"\"\n",
    "###############################################################################################################################\n",
    "# It should be logged in! -> if you have 2fa then you have to authorize it manually and then run the remaining cells manually #\n",
    "###############################################################################################################################\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_location = 'bot_tmp.log'\n",
    "downloaded_files = sorted([os.path.basename(f) for f in glob.glob(secrets[2] + \"/*.pdf\")])\n",
    "\n",
    "#########################################################\n",
    "# sleep is important because if we scrape too fast then #\n",
    "# facebook will detect the bot and block this account   #\n",
    "#########################################################\n",
    "def waitNSeconds(sleep_time = 1):\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "def writeToFile(text):\n",
    "    with open(log_file_location, 'a', encoding='utf-8') as f:\n",
    "        f.write(text + \"\\n\")\n",
    "\n",
    "def isDownloaded(fileName):\n",
    "    # if all files has been checked once then this shouldn't be checked anymore\n",
    "    # because there can be multiple files with the same name and they all should be downloaded\n",
    "    if not downloaded_files: return False\n",
    "\n",
    "    # Binary search -> as the list is sorted\n",
    "    left = 0\n",
    "    right = len(downloaded_files) - 1\n",
    "\n",
    "    while left <= right:\n",
    "        mid = int(left + (right - left) / 2)\n",
    "\n",
    "        if downloaded_files[mid] == fileName:\n",
    "            # remove the found element so that the file with same name doesn't shouldn't exist\n",
    "            downloaded_files.pop(mid)\n",
    "            return True\n",
    "        elif downloaded_files[mid] > fileName:\n",
    "            right = mid - 1\n",
    "        else: left = mid + 1\n",
    "\n",
    "    return False\n",
    "\n",
    "###################################################################\n",
    "# scroll down to load more files                                  #\n",
    "# wait 60s before determining that there is no more files to load #\n",
    "###################################################################\n",
    "def loadMoreFiles(files_to_load, identifier, timeout = 60, n_scroll = 1):\n",
    "    for _ in range(n_scroll): # scroll for n times\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        seconds = 0\n",
    "        current_len = len(files_to_load)\n",
    "\n",
    "        while (current_len >= len(files_to_load)) and seconds < timeout:\n",
    "            time.sleep(1.5) # wait every 1.5s before checking again -> because 1s is too fast :V\n",
    "\n",
    "            files_to_load.extend([\n",
    "                element for element in driver.find_elements(\n",
    "                    By.XPATH, identifier\n",
    "                ) if element not in files_to_load\n",
    "            ])\n",
    "\n",
    "            seconds += 1.5\n",
    "        \n",
    "    return files_to_load\n",
    "\n",
    "def waitToFinishDownload(directory, timeout, nfiles=None):\n",
    "    \"\"\"\n",
    "    Wait for downloads to finish with a specified timeout.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    directory : str\n",
    "        The path to the folder where the files will be downloaded.\n",
    "    timeout : int\n",
    "        How many seconds until it stops waiting.\n",
    "    nfiles : int, defaults to None\n",
    "        If provided, also wait for the expected number of files.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nWaiting for {} files to be downloaded\\n\".format(nfiles))\n",
    "\n",
    "    seconds = 0\n",
    "    dl_wait = True\n",
    "\n",
    "    while dl_wait and seconds < timeout:\n",
    "        time.sleep(0.5) # check every 0.5s\n",
    "        dl_wait = False\n",
    "        files = os.listdir(directory)\n",
    "        \n",
    "        # if nfiles and len(files) != nfiles:\n",
    "        # if nfiles and len(files) < nfiles:\n",
    "        #     dl_wait = True\n",
    "\n",
    "        for fname in files:\n",
    "            if fname.endswith('.crdownload'): # as partial downloaded files will be of \".crdownload\" extension for chromium based browsers\n",
    "                dl_wait = True\n",
    "                break\n",
    "\n",
    "        seconds += 0.5\n",
    "    \n",
    "    if seconds >= timeout:\n",
    "        seconds = -1\n",
    "        print(\"Your connection is too slow or you are not connected! Try again later. Closing the connection...\")\n",
    "    else:\n",
    "        print(\"Continuing after {}s...\\n\".format(seconds))\n",
    "\n",
    "    return seconds\n",
    "\n",
    "# Resume from last downloaded file\n",
    "def checkDownloadHistory(directory, files_in_directory):\n",
    "    with open(directory, 'r', encoding='utf-8') as f:\n",
    "        total_lines = files_in_directory\n",
    "        file_entries = f.readlines()\n",
    "\n",
    "        while total_lines:\n",
    "            total_lines -= 1\n",
    "\n",
    "            try:\n",
    "                last_entry = file_entries[total_lines]\n",
    "                last_entry = str(last_entry).split(' --- ')\n",
    "\n",
    "                # [1:-1] is to strip out the single quote & strip() for extra spaces\n",
    "                last_file_name = last_entry[0].strip()[1:-1]\n",
    "                last_file_date = last_entry[2].strip()[1:-1]\n",
    "\n",
    "                print(\"Download history: Found. Download will resume from '{}' file\".format(last_file_name))\n",
    "                \n",
    "                return last_file_name, last_file_date\n",
    "            except:\n",
    "                last_file_name = False\n",
    "\n",
    "    if not last_file_name: print(\"Download history: Not Found\")\n",
    "\n",
    "    return False, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waitNSeconds(5) # enable this if this script is fully automated for your case\n",
    "driver.get(\"https://www.facebook.com/groups/201623576939858/files/\")\n",
    "\n",
    "download_thread_count = 3 # this value will ensure how many downloads will happen concurrently\n",
    "\n",
    "# to wait until the element is ready -> Explicit Waits\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "##################################################################################\n",
    "# targets identifiers                                                            #\n",
    "# this identifiers will change continuously so update it according to your needs #\n",
    "##################################################################################\n",
    "fileOption_xpath = \"//div[@aria-label='File options']\" # find 15 per-scroll\n",
    "fileName_xpath = \"//span[@class='d2edcug0 hpfvmrgz qv66sw1b c1et5uql lr9zc1uh a8c37x1j fe6kdd0r mau55g9w c8b282yb keod5gw0 nxhoafnm aigsh9s9 d3f4x2em iv3no6db jq4qci2q a3bd9o3v lrazzd5p oo9gr5id hzawbc8m']\" # find 15 per-scroll\n",
    "fileTypeDate_xpath = \"//span[@class='d2edcug0 hpfvmrgz qv66sw1b c1et5uql lr9zc1uh a8c37x1j fe6kdd0r mau55g9w c8b282yb keod5gw0 nxhoafnm aigsh9s9 d9wwppkn iv3no6db e9vueds3 j5wam9gi b1v8xokw oo9gr5id hzawbc8m']\" # find 30 per-scroll\n",
    "# sortButtons_xpath = \"//div[@class='l9j0dhe7 du4w35lb j83agx80 pfnyh3mw taijpn5t bp9cbjyn owycx6da btwxx1t3 kt9q3ron ak7q8e6j isp2s0ed ri5dt5u2 rt8b4zig n8ej3o3l agehan2d sk4xxmp2 rq0escxv d1544ag0 tw6a2znq tdjehn4e tv7at329']\" # find 3 per-scroll\n",
    "\n",
    "#########################################################################\n",
    "# as it doesn't sort by file type reliably ðŸ˜‘\n",
    "# sortButtons = wait.until(\n",
    "#     EC.presence_of_all_elements_located((By.XPATH, sortButtons_xpath))\n",
    "# )\n",
    "\n",
    "# # sort by file_type (PDF at the top in my case)\n",
    "# driver.execute_script(\"arguments[0].click();\", sortButtons[1])\n",
    "#########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial target search\n",
    "file_option_buttons = wait.until(\n",
    "    EC.presence_of_all_elements_located((By.XPATH, fileOption_xpath))\n",
    ")\n",
    "file_names = wait.until(\n",
    "    EC.presence_of_all_elements_located((By.XPATH, fileName_xpath))\n",
    ")\n",
    "file_types_and_dates = wait.until(\n",
    "    EC.presence_of_all_elements_located((By.XPATH, fileTypeDate_xpath))\n",
    ")\n",
    "\n",
    "j, k = 0, 1\n",
    "registered_files_count = sum(1 for line in open(log_file_location, encoding='utf-8')) # total files registered in the log file\n",
    "resume_download = not registered_files_count # if there is log then download will not resume without checking\n",
    "\n",
    "# Resume from last downloaded file\n",
    "if registered_files_count:\n",
    "    last_file_name, last_file_date = checkDownloadHistory(log_file_location, registered_files_count)\n",
    "\n",
    "for idx, button in enumerate(file_option_buttons):\n",
    "    try:\n",
    "        _name = file_names[idx].text\n",
    "        _type = file_types_and_dates[j].text\n",
    "        _date = file_types_and_dates[k].text\n",
    "\n",
    "        # as file_types_and_dates contains both type and date\n",
    "        j += 2; k += 2\n",
    "\n",
    "        # scroll after 15th button click to load more files (as 15 files load per scroll)\n",
    "        if (idx + 1) % 15 == 0:\n",
    "            file_option_buttons = loadMoreFiles(file_option_buttons, fileOption_xpath)\n",
    "\n",
    "            file_names.extend([\n",
    "                name for name in driver.find_elements(\n",
    "                    By.XPATH, fileName_xpath\n",
    "                ) if name not in file_names\n",
    "            ])\n",
    "\n",
    "            file_types_and_dates.extend([\n",
    "                type_date for type_date in driver.find_elements(\n",
    "                    By.XPATH, fileTypeDate_xpath\n",
    "                ) if type_date not in file_types_and_dates\n",
    "            ])\n",
    "\n",
    "        ######################################################################################################################\n",
    "        # download pdf files only                                                                                            #\n",
    "        # cannot check it before the scroll because there is a possibility to have more than one page worth of non-pdf files #\n",
    "        ######################################################################################################################\n",
    "        if not _type == \"PDF\": continue\n",
    "\n",
    "        #################################################################################################################\n",
    "        # to skip already downloaded files                                                                              #\n",
    "        # if first check is False then others will not be checked so you don't have to ->                               #\n",
    "        # worry about having last_file_name, last_file_date being initialized because of resume_download check at first #\n",
    "        #################################################################################################################\n",
    "        if isDownloaded(_name):\n",
    "            print(\"Skipping... {}/{} ---> {}\".format(idx+1, len(file_option_buttons), _name))\n",
    "            continue\n",
    "        \n",
    "        #######################################\n",
    "        if not resume_download and last_file_name and _name == last_file_name and _date == last_file_date:\n",
    "            print(\"Resuming...\\n\")\n",
    "            resume_download = True\n",
    "            continue # for skipping the last entry too\n",
    "\n",
    "        if not resume_download:\n",
    "            print(\"Skipping... {}/{} ---> {}\".format(idx+1, len(file_option_buttons), _name))\n",
    "            continue\n",
    "        #######################################\n",
    "        \n",
    "        ##########################################################################################################\n",
    "        # waiting after every 3 donwloads                                                                        #\n",
    "        # we have to wait for the files to be downloaded, if we continue without it there maybe some files       #\n",
    "        # that couldn't be downloaded on time (before the link expire) and later on those files can't be resumed #\n",
    "        ##########################################################################################################\n",
    "        if (idx + 1) % download_thread_count == 0:\n",
    "            download_time = waitToFinishDownload(secrets[2], 600, registered_files_count) # waiting for 600s before closing the connection for slow/no internet\n",
    "\n",
    "            if download_time == -1: break # stoping the scrapper incase of network failure\n",
    "\n",
    "        ###################################### download script starts ######################################\n",
    "\n",
    "        driver.execute_script(\"arguments[0].click();\", button)\n",
    "        # wait.until(EC.element_to_be_clickable(button)).click() # this was giving me -> ElementClickInterceptedException\n",
    "\n",
    "        waitNSeconds(0.8)\n",
    "\n",
    "        try:\n",
    "            download_link = wait.until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href*='https://www.facebook.com/download/']\"))\n",
    "            )\n",
    "            driver.execute_script(\"arguments[0].target='_self';\", download_link) # to prevent it from opening into a new tab\n",
    "            driver.execute_script(\"arguments[0].click();\", download_link)\n",
    "            \n",
    "            registered_files_count += 1 # keeping track of downloaded files\n",
    "\n",
    "        except:\n",
    "            writeToFile(\"***** Warning: No download link found at {} *****\".format(idx+1))\n",
    "            \n",
    "        ###################################### download script finished ###################################\n",
    "\n",
    "        # updating log\n",
    "        log = '\"{}\" --- \"{}\" --- \"{}\"'.format(_name, _type, _date)\n",
    "        writeToFile(log)\n",
    "\n",
    "        print(\"{} --- {} - {}\".format(_name, _date, len(file_option_buttons)))\n",
    "\n",
    "        #################################################################\n",
    "        # waiting at least 2s for safety sake ðŸ˜…                        #\n",
    "        # waiting (1.2+0.8+calculation_time) -> 2s+ after each download #\n",
    "        #################################################################\n",
    "        waitNSeconds(1.2)\n",
    "\n",
    "    except Exception as e:\n",
    "        log = \"***** ERROR at {}, date: {}: {} *****\".format(idx+1, _date, e)\n",
    "        \n",
    "        print(log)\n",
    "        writeToFile(log)\n",
    "\n",
    "# driver.close() # not closing because I had to run this cell multiple times during test\n",
    "print(\"\\nCompleted. Total scrapped file:\", registered_files_count)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86591fc59d41e1aa02175333bc0753628aa66026b46d3e558ecd962b0f627551"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
